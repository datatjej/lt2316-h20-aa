{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "#extra:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data_dir):\n",
    "    \n",
    "    id2word = {}\n",
    "    id2ner = {0:'none', 1:'group', 2:'drug_n', 3:'drug', 4:'brand'}\n",
    "    data_list = []\n",
    "    ner_list = []\n",
    "    data_dir = glob(\"{}/*\".format(data_dir)) #glob returns a possibly-empty list of path names that match data_dir \n",
    "                                            #...in this case a list with the two subdirectories 'Test' and 'Train'                                           \n",
    "    for subdir in data_dir: #looping through 'Test' and 'Train'\n",
    "        split = os.path.basename(subdir) #get the directory name without path\n",
    "        subdir = glob(\"{}/*\".format(subdir))\n",
    "        if split == 'Train':\n",
    "            for folder in subdir:\n",
    "                folder = glob(\"{}/*\".format(folder))\n",
    "                for xml_file in folder:\n",
    "                    token_instances, ner_instances, id2word = parse_xml(xml_file, split, id2word, id2ner)\n",
    "                    #print(\"NER_INSTANCES (Train): \", ner_instances)\n",
    "                    data_list = data_list + token_instances\n",
    "                    for instance in ner_instances:\n",
    "                        if instance:\n",
    "                            ner_list.append(instance)\n",
    "        elif split == 'Test':\n",
    "            for folder in subdir:  #looping through 'Test for DDI Extraction task' and 'Test for DrugNER task'\n",
    "                folder = glob(\"{}/*\".format(folder))\n",
    "                for subfolder in folder: #looping through 'DrugBank' and 'MedLine'\n",
    "                    subfolder = glob(\"{}/*\".format(subfolder))\n",
    "                    for xml_file in subfolder:\n",
    "                        token_instances, ner_instances, id2word = parse_xml(xml_file, split, id2word, id2ner)\n",
    "                        #print(\"NER_INSTANCES (Test): \", ner_instances)\n",
    "                        data_list = data_list + token_instances\n",
    "                        for instance in ner_instances:\n",
    "                            if instance:\n",
    "                                ner_list.append(instance)\n",
    "    \n",
    "    vocab = list(id2word.values()) #keeping track of unique words in the data\n",
    "    data_df, ner_df = list2df(data_list, ner_list) #turn lists into dataframes\n",
    "    #df1 = data_df[data_df.isnull().any(axis=1)]\n",
    "    #display(df1)\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #    display(data_df)\n",
    "    return data_df, ner_df\n",
    "                    \n",
    "def list2df(data_list, ner_list):\n",
    "    data_df = pd.DataFrame.from_records(data_list, columns=['sentence_id', 'token_id', 'char_start_id', 'char_end_id', 'split'])\n",
    "    #data_df = data_df[~data_df['token'].isin(list(string.punctuation))] #remove tokens that are just punctuation \n",
    "    #data_df.drop('token', inplace=True, axis=1) #remove 'token' column since it's not needed anymore\n",
    "    #'inPlace=True' means we are working on the original df, 'axis=1' refers to the column axis\n",
    "    train_samples = data_df[data_df['split']=='Train'].sample(frac=0.15) #sample 15 % of 'Train'-labeled rows\n",
    "    train_samples.split='Val' #replace those 'Train' labels with 'Val'\n",
    "    data_df.update(train_samples) #incorporate the modified train samples back into the original dataframe\n",
    "    ner_df = pd.DataFrame.from_records(ner_list, columns=['sentence_id', 'ner_id', 'char_start_id', 'char_end_id'])\n",
    "    return data_df, ner_df    \n",
    "                        \n",
    "def parse_xml(xml_file, split, id2word, id2ner):\n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    token_instances = [] #save all token \n",
    "    ner_instances = []\n",
    "    \n",
    "    for elem in root: #loop over sentence tags\n",
    "        if elem.tag == 'sentence':\n",
    "            sent_id = elem.attrib['id'] #get sentence id\n",
    "            text = elem.attrib['text']  #get the sentence as a string of text\n",
    "            text = text.replace('-', ' ') #replaces all hyphens with whitespace for easier split of compound words\n",
    "            char_pos = -1 #variable for keeping track of character-based positions of the words in the sentence\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            for token in nltk_tokens:\n",
    "                char_pos, token_instance, id2word  = get_token_instance(char_pos, sent_id, token, split, id2word)\n",
    "                token_instances.append(token_instance)\n",
    "        for subelem in elem: #looping through children tags (i.e. 'entity', 'pair') of sentence_id\n",
    "            if subelem.tag == 'entity':\n",
    "                ner_instance = get_ner_instance(sent_id, subelem, id2ner)\n",
    "                #print(\"ner_instance (parse_xml): \", ner_instance)\n",
    "                for instance in ner_instance: #loop through list of returned NER instances\n",
    "                    ner_instances.append(instance) #save them individually in the ner_instances list\n",
    "    #print(\"NER_INSTANCES: \", ner_instances)\n",
    "    return token_instances, ner_instances, id2word\n",
    "\n",
    "def get_token_instance(char_pos, sent_id, token, split, id2word):\n",
    "    char_pos += 1\n",
    "    char_start = char_pos\n",
    "    char_end = char_start + len(token)-1\n",
    "    token_id, id2word = map_token_to_id(token, id2word)\n",
    "    token_instance = [sent_id, int(token_id), int(char_start), int(char_end), split]\n",
    "    #print(\"TOKEN INSTANCE: \", token_instance)(\n",
    "    char_pos=char_end+1 #increase by 1 to account for the whitespace between the current and the next word\n",
    "    return char_pos, token_instance, id2word\n",
    "\n",
    "def get_ner_id_as_int(ner_id, id2ner):\n",
    "    for key, value in id2ner.items(): \n",
    "         if ner_id == value: \n",
    "            return key \n",
    "    else:\n",
    "        return \"key doesn't exist\"\n",
    "    \n",
    "\n",
    "def get_ner_instance(sent_id, entity, id2ner):\n",
    "    #Problem of this approach: if a NER might be tokenized differently from the token dataframe\n",
    "    ner_instances = []\n",
    "    #ner_token = entity.attrib['text']\n",
    "    #tokenized_ner = entity.attrib['text'].split() \n",
    "    charOffset = entity.attrib['charOffset']\n",
    "    #HAPPY PATH: if the character span is a single span:\n",
    "    if ';' not in charOffset:\n",
    "        char_start = charOffset.split('-')[0]\n",
    "        char_end = charOffset.split('-')[1]\n",
    "        ner_id = get_ner_id_as_int(entity.attrib['type'], id2ner)\n",
    "        #ner_id = entity.attrib['type'] #getting the label: 'brand', 'drug', 'drug_n' or 'group'\n",
    "        ner_instance = [sent_id, ner_id, char_start, char_end]\n",
    "        return [ner_instance]\n",
    "    #PATH OF DOOM: for multiword entities with several character spans:\n",
    "    if ';' in charOffset:\n",
    "        for span in charOffset.split(';'):\n",
    "            ner_id = entity.attrib['type'] #getting the label: 'brand', 'drug', 'drug_n' or 'group'\n",
    "            char_start = span.split('-')[0]\n",
    "            char_end = span.split('-')[1]\n",
    "            ner_instance = [sent_id, ner_id, int(char_start), int(char_end)]\n",
    "            ner_instances.append(ner_instance)\n",
    "            #print(\"SPECIAL NER_INSTANCE: \", ner_instance)\n",
    "    return ner_instances\n",
    "    \n",
    "def map_token_to_id(token, id2word):\n",
    "    res = False\n",
    "    for key in id2word: \n",
    "        if(id2word[key] == token):\n",
    "            res = True\n",
    "            return key, id2word\n",
    "    if res == False:\n",
    "        token_id = len(id2word)+1\n",
    "        id2word[token_id] = token\n",
    "        return token_id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 sentence_id  token_id  char_start_id  char_end_id  split\n",
       " 0       DDI-DrugBank.d610.s0       1.0            0.0         14.0   Test\n",
       " 1       DDI-DrugBank.d610.s0       2.0           16.0         25.0   Test\n",
       " 2       DDI-DrugBank.d610.s0       3.0           27.0         28.0   Test\n",
       " 3       DDI-DrugBank.d610.s0       4.0           30.0         37.0   Test\n",
       " 4       DDI-DrugBank.d610.s0       5.0           39.0         42.0   Test\n",
       " ...                      ...       ...            ...          ...    ...\n",
       " 195940   DDI-MedLine.d113.s6     636.0          107.0        112.0    Val\n",
       " 195941   DDI-MedLine.d113.s6       3.0          114.0        115.0  Train\n",
       " 195942   DDI-MedLine.d113.s6     372.0          117.0        120.0  Train\n",
       " 195943   DDI-MedLine.d113.s6     510.0          122.0        126.0  Train\n",
       " 195944   DDI-MedLine.d113.s6      17.0          128.0        128.0  Train\n",
       " \n",
       " [195945 rows x 5 columns],\n",
       "                 sentence_id ner_id char_start_id char_end_id\n",
       " 0      DDI-DrugBank.d610.s0      3            30          37\n",
       " 1      DDI-DrugBank.d610.s0      3            82          91\n",
       " 2      DDI-DrugBank.d610.s0      3            96         105\n",
       " 3      DDI-DrugBank.d610.s0      3           129         138\n",
       " 4      DDI-DrugBank.d610.s0      3           144         153\n",
       " ...                     ...    ...           ...         ...\n",
       " 18536   DDI-MedLine.d113.s3      1           118         131\n",
       " 18537   DDI-MedLine.d113.s3      3           138         144\n",
       " 18538   DDI-MedLine.d113.s5      3           111         117\n",
       " 18539   DDI-MedLine.d113.s5      1           123         150\n",
       " 18540   DDI-MedLine.d113.s6      3            19          25\n",
       " \n",
       " [18541 rows x 4 columns])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_data('/home/guserbto@GU.GU.SE/lt2316-h20-aa/DDICorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
