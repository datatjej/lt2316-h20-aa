{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "#extra:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data_dir):\n",
    "    \n",
    "    id2word = {}\n",
    "    id2ner = {0:'none', 1:'group', 2:'drug_n', 3:'drug', 4:'brand'}\n",
    "    data_list = []\n",
    "    ner_list = []\n",
    "    data_dir = glob(\"{}/*\".format(data_dir)) #glob returns a possibly-empty list of path names that match data_dir \n",
    "                                            #...in this case a list with the two subdirectories 'Test' and 'Train'                                           \n",
    "    for subdir in data_dir: #looping through 'Test' and 'Train'\n",
    "        split = os.path.basename(subdir) #get the directory name without path\n",
    "        subdir = glob(\"{}/*\".format(subdir))\n",
    "        if split == 'Train':\n",
    "            for folder in subdir:\n",
    "                folder = glob(\"{}/*\".format(folder))\n",
    "                for xml_file in folder:\n",
    "                    token_instances, ner_instances, id2word = parse_xml(xml_file, split, id2word, id2ner)\n",
    "                    #print(\"NER_INSTANCES (Train): \", ner_instances)\n",
    "                    data_list = data_list + token_instances\n",
    "                    for instance in ner_instances:\n",
    "                        if instance:\n",
    "                            ner_list.append(instance)\n",
    "        elif split == 'Test':\n",
    "            for folder in subdir:  #looping through 'Test for DDI Extraction task' and 'Test for DrugNER task'\n",
    "                folder = glob(\"{}/*\".format(folder))\n",
    "                for subfolder in folder: #looping through 'DrugBank' and 'MedLine'\n",
    "                    subfolder = glob(\"{}/*\".format(subfolder))\n",
    "                    for xml_file in subfolder:\n",
    "                        token_instances, ner_instances, id2word = parse_xml(xml_file, split, id2word, id2ner)\n",
    "                        #print(\"NER_INSTANCES (Test): \", ner_instances)\n",
    "                        data_list = data_list + token_instances\n",
    "                        for instance in ner_instances:\n",
    "                            if instance:\n",
    "                                ner_list.append(instance)\n",
    "    \n",
    "    vocab = list(id2word.values()) #keeping track of unique words in the data\n",
    "    data_df, ner_df = list2df(data_list, ner_list) #turn lists into dataframes\n",
    "    #df1 = data_df[data_df.isnull().any(axis=1)]\n",
    "    #display(data_df)\n",
    "    #display(ner_df)\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #    display(data_df)\n",
    "    return data_df, ner_df\n",
    "                    \n",
    "def list2df(data_list, ner_list):\n",
    "    data_df = pd.DataFrame.from_records(data_list, columns=['sentence_id', 'token_id', 'char_start_id', 'char_end_id', 'split'])\n",
    "    #data_df = data_df[~data_df['token'].isin(list(string.punctuation))] #remove tokens that are just punctuation \n",
    "    #data_df.drop('token', inplace=True, axis=1) #remove 'token' column since it's not needed anymore\n",
    "    #'inPlace=True' means we are working on the original df, 'axis=1' refers to the column axis\n",
    "    #train_samples = data_df[data_df['split']=='Train'].sample(frac=0.15) #sample 15 % of 'Train'-labeled rows --> SENTENCES\n",
    "    #train_samples.split='Val' #replace those 'Train' labels with 'Val'\n",
    "    #data_df.update(train_samples) #incorporate the modified train samples back into the original dataframe\n",
    "    train_df = data_df[data_df['split']=='Train']\n",
    "    unique_sent_in_train = list(train_df['sentence_id'].unique())\n",
    "    print(\"UNIQUE TRAIN SENT: \", len(unique_sent_in_train))\n",
    "    val_sample_sentences = unique_sent_in_train[:int(len(unique_sent_in_train) * .15)]\n",
    "    print(\"UNIQUE VAL SENT: \", len(val_sample_sentences))\n",
    "    val_df = data_df[data_df['sentence_id'].isin(val_sample_sentences)]\n",
    "    val_df.split='Val'\n",
    "    data_df.update(val_df)\n",
    "    val_df = data_df[data_df.split=='Val']\n",
    "    ner_df = pd.DataFrame.from_records(ner_list, columns=['sentence_id', 'ner_id', 'char_start_id', 'char_end_id'])\n",
    "    counts = ner_df[\"ner_id\"].value_counts()\n",
    "    print(counts)\n",
    "    return data_df, ner_df    \n",
    "                        \n",
    "def parse_xml(xml_file, split, id2word, id2ner):\n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    token_instances = [] #save all token \n",
    "    ner_instances = []\n",
    "    \n",
    "    for elem in root: #loop over sentence tags\n",
    "        if elem.tag == 'sentence':\n",
    "            sent_id = elem.attrib['id'] #get sentence id\n",
    "            text = elem.attrib['text']  #get the sentence as a string of text\n",
    "            text = text.replace('-', ' ') #replaces all hyphens with whitespace for easier split of compound words\n",
    "            char_pos = -1 #variable for keeping track of character-based positions of the words in the sentence\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            for token in nltk_tokens:\n",
    "                char_pos, token_instance, id2word  = get_token_instance(char_pos, sent_id, token, split, id2word)\n",
    "                token_instances.append(token_instance)\n",
    "        for subelem in elem: #looping through children tags (i.e. 'entity', 'pair') of sentence_id\n",
    "            if subelem.tag == 'entity':\n",
    "                ner_instance = get_ner_instance(sent_id, subelem, id2ner)\n",
    "                #print(\"ner_instance (parse_xml): \", ner_instance)\n",
    "                for instance in ner_instance: #loop through list of returned NER instances\n",
    "                    ner_instances.append(instance) #save them individually in the ner_instances list\n",
    "    #print(\"NER_INSTANCES: \", ner_instances)\n",
    "    return token_instances, ner_instances, id2word\n",
    "\n",
    "def get_token_instance(char_pos, sent_id, token, split, id2word):\n",
    "    char_pos += 1\n",
    "    char_start = char_pos\n",
    "    char_end = char_start + len(token)-1\n",
    "    token_id, id2word = map_token_to_id(token, id2word)\n",
    "    token_instance = [sent_id, int(token_id), int(char_start), int(char_end), split]\n",
    "    #print(\"TOKEN INSTANCE: \", token_instance)(\n",
    "    char_pos=char_end+1 #increase by 1 to account for the whitespace between the current and the next word\n",
    "    return char_pos, token_instance, id2word\n",
    "\n",
    "def get_ner_id_as_int(ner_id, id2ner):\n",
    "    for key, value in id2ner.items(): \n",
    "         if ner_id == value: \n",
    "            return key \n",
    "    else:\n",
    "        return \"key doesn't exist\"\n",
    "    \n",
    "\n",
    "def get_ner_instance(sent_id, entity, id2ner):\n",
    "    #Problem of this approach: if a NER might be tokenized differently from the token dataframe\n",
    "    ner_instances = []\n",
    "    #ner_token = entity.attrib['text']\n",
    "    #tokenized_ner = entity.attrib['text'].split() \n",
    "    charOffset = entity.attrib['charOffset']\n",
    "    #HAPPY PATH: if the character span is a single span:\n",
    "    if ';' not in charOffset:\n",
    "        char_start = charOffset.split('-')[0]\n",
    "        char_end = charOffset.split('-')[1]\n",
    "        ner_id = get_ner_id_as_int(entity.attrib['type'], id2ner)\n",
    "        #ner_id = entity.attrib['type'] #getting the label: 'brand', 'drug', 'drug_n' or 'group'\n",
    "        ner_instance = [sent_id, int(ner_id), int(char_start), int(char_end)]\n",
    "        return [ner_instance]\n",
    "    #PATH OF DOOM: for multiword entities with several character spans:\n",
    "    if ';' in charOffset:\n",
    "        for span in charOffset.split(';'):\n",
    "            ner_id = get_ner_id_as_int(entity.attrib['type'], id2ner) #getting the label: 'brand', 'drug', 'drug_n' or 'group'\n",
    "            char_start = span.split('-')[0]\n",
    "            char_end = span.split('-')[1]\n",
    "            ner_instance = [sent_id, int(ner_id), int(char_start), int(char_end)]\n",
    "            ner_instances.append(ner_instance)\n",
    "            #print(\"SPECIAL NER_INSTANCE: \", ner_instance)\n",
    "    return ner_instances\n",
    "    \n",
    "def map_token_to_id(token, id2word):\n",
    "    res = False\n",
    "    for key in id2word: \n",
    "        if(id2word[key] == token):\n",
    "            res = True\n",
    "            return key, id2word\n",
    "    if res == False:\n",
    "        token_id = len(id2word)+1\n",
    "        id2word[token_id] = token\n",
    "        return token_id, id2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE TRAIN SENT:  6905\n",
      "UNIQUE VAL SENT:  1035\n",
      "3    11656\n",
      "1     4254\n",
      "4     1865\n",
      "2      766\n",
      "Name: ner_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/pandas/core/generic.py:5209: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "data_df, ner_df = parse_data('/home/guserbto@GU.GU.SE/lt2316-h20-aa/DDICorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(data_df, ner_df):\n",
    "    # Should return a tensor containing the ner labels for all samples in each split.\n",
    "    # the tensors should have the following following dimensions:\n",
    "    # (NUMBER_SAMPLES, MAX_SAMPLE_LENGTH)\n",
    "    # NOTE! the labels for each split should be on the GPU\n",
    "    device = torch.device('cuda:1')\n",
    "    \n",
    "    # divide df by splits\n",
    "    df_train = data_df[data_df.split=='Train']\n",
    "    print(\"Unique sent in Train: \", len(list(df_train['sentence_id'].unique()))) \n",
    "    #print(\"df_train size: \", df_train.size)\n",
    "    df_val = data_df[data_df.split=='Val']\n",
    "    #print(\"df_val size: \", df_val.size)\n",
    "    print(\"Unique sent in Val: \", len(list(df_val['sentence_id'].unique()))) \n",
    "    df_test = data_df[data_df.split=='Test']\n",
    "    #print(\"df_test size: \", df_test.size)\n",
    "    print(\"Unique sent in Test: \", len(list(df_test['sentence_id'].unique()))) \n",
    "    \n",
    "    max_sample_length, sample_lengths_dict = get_sample_lengths(data_df)\n",
    "    \n",
    "    #get labels\n",
    "    train_labels = label_tokens(df_train, ner_df, max_sample_length, sample_lengths_dict)\n",
    "    #print(\"train labels:\", len(train_labels))\n",
    "    train_tensor = torch.LongTensor(train_labels)\n",
    "    train_tensor = train_tensor.to(device)\n",
    "    \n",
    "    test_labels = label_tokens(df_test, ner_df, max_sample_length, sample_lengths_dict)\n",
    "    test_tensor = torch.LongTensor(test_labels)\n",
    "    test_tensor = test_tensor.to(device)\n",
    "    \n",
    "    val_labels = label_tokens(df_val, ner_df, max_sample_length, sample_lengths_dict)\n",
    "    val_tensor = torch.LongTensor(val_labels)\n",
    "    val_tensor = val_tensor.to(device)\n",
    "  \n",
    "    print(\"val labels:\", len(val_labels))\n",
    "    print(\"test labels:\", len(test_labels))\n",
    "    print(\"test labels:\", len(train_labels))\n",
    "\n",
    "\n",
    "    return train_tensor, val_tensor, test_tensor\n",
    "    #return val_tensor\n",
    "\n",
    "def label_tokens(df, ner_df, max_sample_length, sample_lengths_dict):\n",
    "    labels = []\n",
    "    \n",
    "    df_as_list = df.values.tolist()\n",
    "    #print(\"df_as_list[:50]: \", df_as_list[:50])\n",
    "    ner_df_as_list = ner_df.values.tolist()\n",
    "    \n",
    "    #max_sample_length = 165\n",
    "    \n",
    "    sentence_labels = []\n",
    "    match_found_count = 0\n",
    "    for df_row in df_as_list:\n",
    "        sentence_id = df_row[0]\n",
    "        sentence_length = sample_lengths_dict[sentence_id]\n",
    "        #print(\"SENTENCE_LENGTH: \", sentence_length)\n",
    "        match_found = False \n",
    "        for ner_row in ner_df_as_list:\n",
    "            #compare sentence_id, char_start, char_end between df_row and ner_rows: \n",
    "            if df_row[0] == ner_row[0]:\n",
    "                if int(df_row[2]) == ner_row[2] and int(df_row[3]) == ner_row[3]:\n",
    "                    label = ner_row[1]\n",
    "                    match_found = True\n",
    "                    #print(\"match found\", df_row, \"<3\", ner_row) \n",
    "                    sentence_labels.append(label)\n",
    "                    #continue\n",
    "        if match_found == False:\n",
    "            label = 0\n",
    "            sentence_labels.append(label)\n",
    "        if len(sentence_labels) == sentence_length:\n",
    "            #print(\"************SENTENCE DONE************\")\n",
    "            #print(\"SENTENCE_LABELS BEFORE PADDING: \", sentence_labels)\n",
    "            padded_sentence_labels = get_padding(sentence_labels, max_sample_length)\n",
    "            #print(\"PADDED SENTENCE LABELS: \", sentence_labels)\n",
    "            #if padded_sentence_labels is not None:\n",
    "            labels.append(padded_sentence_labels)\n",
    "            #print(\"LABELS AFTER ADDING PADDED SENT: \", labels)\n",
    "            sentence_labels = []\n",
    "            #else:\n",
    "            #    print(\"omg it's a none\", sentence_id)\n",
    "    #print(\"LABELS BEFORE RETURN: \", labels)\n",
    "            \n",
    "    return labels \n",
    "\n",
    "def get_sample_lengths(data_df):\n",
    "    max_sample_length = max(data_df.groupby('sentence_id').size())\n",
    "    sample_lengths = data_df.groupby('sentence_id').size().tolist() \n",
    "    unique_sentences = data_df['sentence_id'].unique() \n",
    "    sentences_list = sorted(unique_sentences) \n",
    "    sample_length_dict = {sentences_list[i]: sample_lengths[i] for i in range(len(sentences_list))} \n",
    "    #display(data_df.groupby('sentence_id').size().nlargest(5))\n",
    "    #display(data_df.groupby('sentence_id').size()) \n",
    "    return max_sample_length, sample_length_dict\n",
    "\n",
    "def get_padding(sentence_labels, max_sample_length):\n",
    "    #print(\"SENTENCE LABELS: \", sentence_labels)\n",
    "    diff = max_sample_length - len(sentence_labels)\n",
    "    #print(\"DIFF: \", diff)\n",
    "    if int(diff) == 0:\n",
    "        print(\"SENTENCE WITH NO DIFF: \", sentence_labels, \"DIFF: \", diff)\n",
    "        return sentence_labels\n",
    "    else:\n",
    "        padding = [0] * diff\n",
    "        sentence_labels.extend(padding)\n",
    "    #print(\"PADDED SENTENCE_LABELS BEFORE RETURN: \", sentence_labels)\n",
    "    #if diff == 0:\n",
    "    #    padded_sentence_labels = sentence_labels\n",
    "    #    print(\"PAD_SENT AFTER DIFF==0: \", padded_sentence_labels)\n",
    "    #else: \n",
    "    #padded_sentence_labels = sentence_labels.extend(padding)\n",
    "    #print(\"PADDED_SENT BEFORE RETURN: \", padded_sentence_labels)\n",
    "    #print(\"PADDED SENTENCE LABELS: \", padded_sentence_labels)\n",
    "    return sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sent in Train:  5870\n",
      "Unique sent in Val:  1035\n",
      "Unique sent in Test:  1964\n",
      "SENTENCE WITH NO DIFF:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] DIFF:  0\n",
      "val labels: 1035\n",
      "test labels: 1684\n",
      "test labels: 5870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [3, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_y(data_df, ner_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
