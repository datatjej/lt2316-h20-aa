{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "#extra:\n",
    "import os\n",
    "import nltk\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_id : id for the sentence, can be found in the xml data\n",
    "#token_id: id for the word in the vocabulary\n",
    "#char_start_id: where the character starts in the sentence, can be found in the xml data\n",
    "#char_end_id: where the character ends in the sentence, can be found in the xml data\n",
    "#split: which data split the token belong to, e.g. TRAIN, VAL or TEST.\n",
    "#ner_id: id of the NER label, e.g. if we have 3 labels there would be 3 ids.from glob import glob\n",
    "\n",
    "def parse_data(data_dir):\n",
    "    \n",
    "    data_list = []\n",
    "    ner_list = []\n",
    "    vocab = [] #keeping track of unique words in the data\n",
    "    data_dir = glob(\"{}/*\".format(data_dir)) #glob returns a possibly-empty list of path names that match data_dir \n",
    "                                            #...in this case a list with the two subdirectories 'Test' and 'Train'                                           \n",
    "    for subdir in data_dir: #looping through 'Test' and 'Train'\n",
    "        split = os.path.basename(subdir) #get the directory name without path\n",
    "        subdir = glob(\"{}/*\".format(subdir))\n",
    "        for folder in subdir:  #looping through 'Test for DDI Extraction task' and 'Test for DrugNER task'\n",
    "            folder = glob(\"{}/*\".format(folder))\n",
    "            for subfolder in folder: #looping through 'DrugBank' and 'MedLine'\n",
    "                subfolder = glob(\"{}/*\".format(subfolder))\n",
    "                for xml_file in subfolder:\n",
    "                    token_instances, ner_instances, vocab = parse_xml(xml_file, split, vocab)\n",
    "                    data_list = handle_lists(token_instances, data_list)\n",
    "                    ner_list = handle_lists(ner_instances, ner_list)\n",
    "\n",
    "def handle_lists(returned_instances, existing_list):\n",
    "    if not existing_list: #check if list is empty\n",
    "        existing_list = returned_instances #if it is, then populate it with the newly returned instances\n",
    "    else:\n",
    "        existing_list = existing_list + returned_instances #else concatenate the existing list with the newly returned instance list\n",
    "    return existing_list\n",
    "                        \n",
    "def parse_xml(xml_file, split, vocab):    \n",
    "    \n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    token_instances = [] \n",
    "    ner_instances = []\n",
    "    \n",
    "    for elem in root: #loop over sentence tags\n",
    "        if elem.tag == 'sentence':\n",
    "            sent_id = elem.attrib['id'] #get sentence id\n",
    "            text = elem.attrib['text']  #get the sentence as a string of text\n",
    "            text = text.replace('-', ' ') #replaces all hyphens with whitespace for easier split of compound words\n",
    "            char_pos = -1 #variable for keeping track of character-based positions of the words in the sentence\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            for token in nltk_tokens:\n",
    "                char_pos += 1\n",
    "                char_start = char_pos\n",
    "                char_end = char_start + len(token)-1\n",
    "                token_id, vocab = map_token_to_id(token, vocab)\n",
    "                #TODO: ta bort 'token' efter testning:\n",
    "                token_instance = [sent_id, token, token_id, char_start, char_end, split]\n",
    "                token_instances.append(token_instance)\n",
    "                char_pos=char_end+1 #increase by 1 to account for the whitespace between the current and the next word \n",
    "        for subelem in elem: #looping through children of sentence_id, such as 'entity' and 'pair' \n",
    "            if subelem.tag == 'entity':\n",
    "                ner_id = subelem.attrib['text']\n",
    "                char_start = subelem.attrib['charOffset'].split('-')[0]\n",
    "                char_end = subelem.attrib['charOffset'].split('-')[1]\n",
    "                ner_instance = [sent_id, ner_id, char_start, char_end]\n",
    "                ner_instances.append(ner_instance)\n",
    "    return token_instances, ner_instances, vocab\n",
    "    \n",
    "def map_token_to_id(token, vocab):\n",
    "    vocab = vocab\n",
    "    if token not in vocab:\n",
    "        vocab.append(token)\n",
    "    token_id = vocab.index(token)\n",
    "    return token_id, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDI-DrugBank.d610.s0', 'abacavir', '30', '37']\n"
     ]
    }
   ],
   "source": [
    "parse_data('/home/guserbto@GU.GU.SE/lt2316-h20-aa/DDICorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
