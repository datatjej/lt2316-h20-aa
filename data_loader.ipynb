{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "#extra:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_id : id for the sentence, can be found in the xml data\n",
    "#token_id: id for the word in the vocabulary\n",
    "#char_start_id: where the character starts in the sentence, can be found in the xml data\n",
    "#char_end_id: where the character ends in the sentence, can be found in the xml data\n",
    "#split: which data split the token belong to, e.g. TRAIN, VAL or TEST.\n",
    "#ner_id: id of the NER label, e.g. if we have 3 labels there would be 3 ids.from glob import glob\n",
    "\n",
    "def parse_data(data_dir):\n",
    "    \n",
    "    data_list = []\n",
    "    ner_list = []\n",
    "    vocab = [] #keeping track of unique words in the data\n",
    "    data_dir = glob(\"{}/*\".format(data_dir)) #glob returns a possibly-empty list of path names that match data_dir \n",
    "                                            #...in this case a list with the two subdirectories 'Test' and 'Train'                                           \n",
    "    for subdir in data_dir: #looping through 'Test' and 'Train'\n",
    "        split = os.path.basename(subdir) #get the directory name without path\n",
    "        subdir = glob(\"{}/*\".format(subdir))\n",
    "        for folder in subdir:  #looping through 'Test for DDI Extraction task' and 'Test for DrugNER task'\n",
    "            folder = glob(\"{}/*\".format(folder))\n",
    "            for subfolder in folder: #looping through 'DrugBank' and 'MedLine'\n",
    "                subfolder = glob(\"{}/*\".format(subfolder))\n",
    "                for xml_file in subfolder:\n",
    "                    token_instances, ner_instances, vocab = parse_xml(xml_file, split, vocab)\n",
    "                    data_list = handle_lists(token_instances, data_list)\n",
    "                    ner_list = handle_lists(ner_instances, ner_list)\n",
    "    \n",
    "    data_df, ner_df = list2df(data_list, ner_list) #turn lists into dataframes\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(data_df)\n",
    "        \n",
    "    #return display(data_df), print(ner_df)\n",
    "                    \n",
    "def list2df(data_list, ner_list):\n",
    "    data_df = pd.DataFrame.from_records(data_list, columns=['sentence_id', 'token', 'token_id', 'char_start_id', 'char_end_id', 'split'])\n",
    "    data_df = data_df[~data_df['token'].isin(list(string.punctuation))] #remove tokens that are just punctuation \n",
    "    data_df.drop('token', inplace=True, axis=1) #remove whole 'token' column since it's not needed anymore\n",
    "    #'inPlace=True' means we are working on the original df, 'axis=1' refers to column\n",
    "    \n",
    "    #count number of 'Train' rows\n",
    "    #replace 15 % of Train labels with 'Val' at random\n",
    "    \n",
    "    ner_df = pd.DataFrame.from_records(ner_list, columns=['sentence_id', 'ner_id', 'char_start_id', 'char_end_id'])\n",
    "    return data_df, ner_df    \n",
    "                    \n",
    "def handle_lists(returned_instances, existing_list):\n",
    "    if not existing_list: #check if list is empty\n",
    "        existing_list = returned_instances #if it is, then populate it with the newly returned instances\n",
    "    else:\n",
    "        existing_list = existing_list + returned_instances #else concatenate the existing list with the newly returned instance list\n",
    "    return existing_list\n",
    "                        \n",
    "def parse_xml(xml_file, split, vocab):    \n",
    "    \n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    token_instances = [] \n",
    "    ner_instances = []\n",
    "    \n",
    "    for elem in root: #loop over sentence tags\n",
    "        if elem.tag == 'sentence':\n",
    "            sent_id = elem.attrib['id'] #get sentence id\n",
    "            text = elem.attrib['text']  #get the sentence as a string of text\n",
    "            text = text.replace('-', ' ') #replaces all hyphens with whitespace for easier split of compound words\n",
    "            char_pos = -1 #variable for keeping track of character-based positions of the words in the sentence\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            for token in nltk_tokens:\n",
    "                char_pos, token_instance, vocab  = get_token_instance(char_pos, sent_id, token, split, vocab)\n",
    "                token_instances.append(token_instance)\n",
    "        for subelem in elem: #looping through children of sentence_id, such as 'entity' and 'pair' \n",
    "            if subelem.tag == 'entity':\n",
    "                ner_instance = get_ner_instance(sent_id, subelem)\n",
    "                ner_instances.append(ner_instance)\n",
    "    return token_instances, ner_instances, vocab\n",
    "\n",
    "def get_token_instance(char_pos, sent_id, token, split, vocab):\n",
    "    char_pos += 1\n",
    "    char_start = char_pos\n",
    "    char_end = char_start + len(token)-1\n",
    "    token_id, vocab = map_token_to_id(token, vocab)\n",
    "    #TODO: ta bort 'token' efter dataframe-skapande\n",
    "    token_instance = [sent_id, token, token_id, char_start, char_end, split]\n",
    "    #print(\"TOKEN INSTACE: \", token_instance)\n",
    "    char_pos=char_end+1 #increase by 1 to account for the whitespace between the current and the next word\n",
    "    return char_pos, token_instance, vocab\n",
    "\n",
    "def get_ner_instance(sent_id, entity):\n",
    "    #TODO: take multispan NERs into consideration adding \n",
    "    #hyphen_count = entity.attrib['charOffset'].count('-')\n",
    "    ner_id = entity.attrib['type']\n",
    "    char_start = entity.attrib['charOffset'].split('-')[0]\n",
    "    char_end = entity.attrib['charOffset'].split('-')[1]\n",
    "    ner_instance = [sent_id, ner_id, char_start, char_end]\n",
    "    #print(\"NER_INSTANCE: \", ner_instance)\n",
    "    return ner_instance\n",
    "    \n",
    "def map_token_to_id(token, vocab):\n",
    "    vocab = vocab\n",
    "    if token not in vocab:\n",
    "        vocab.append(token)\n",
    "    token_id = vocab.index(token)\n",
    "    return token_id, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sentence_id  token_id  char_start_id  char_end_id split\n",
      "0      DDI-DrugBank.d610.s0         0              0           14  Test\n",
      "1      DDI-DrugBank.d610.s0         1             16           25  Test\n",
      "2      DDI-DrugBank.d610.s0         2             27           28  Test\n",
      "3      DDI-DrugBank.d610.s0         3             30           37  Test\n",
      "4      DDI-DrugBank.d610.s0         4             39           42  Test\n",
      "...                     ...       ...            ...          ...   ...\n",
      "45551   DDI-MedLine.d153.s8       765            208          209  Test\n",
      "45552   DDI-MedLine.d153.s8        57            211          212  Test\n",
      "45553   DDI-MedLine.d153.s8        67            214          217  Test\n",
      "45554   DDI-MedLine.d153.s8        40            219          220  Test\n",
      "45555   DDI-MedLine.d153.s8       108            222          225  Test\n",
      "\n",
      "[39620 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "parse_data('/home/guserbto@GU.GU.SE/lt2316-h20-aa/DDICorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
