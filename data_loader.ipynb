{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "#extra:\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from glob import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data_dir):\n",
    "    \n",
    "    id2word = {}\n",
    "    id2ner = {0:'none', 1:'group', 2:'drug_n', 3:'drug', 4:'brand'}\n",
    "    data_list = []\n",
    "    ner_list = []\n",
    "    data_dir = glob(\"{}/*\".format(data_dir)) #glob returns a possibly-empty list of path names that match data_dir \n",
    "                                            #...in this case a list with the two subdirectories 'Test' and 'Train'                                           \n",
    "    for subdir in data_dir: #looping through 'Test' and 'Train'\n",
    "        split = os.path.basename(subdir) #get the directory name without path\n",
    "        subdir = glob(\"{}/*\".format(subdir))\n",
    "        if split == 'Train':\n",
    "            for folder in subdir:\n",
    "                folder = glob(\"{}/*\".format(folder))\n",
    "                for xml_file in folder:\n",
    "                    token_instances, ner_instances, id2word = parse_xml(xml_file, split, id2word, id2ner)\n",
    "                    #print(\"NER_INSTANCES (Train): \", ner_instances)\n",
    "                    data_list = data_list + token_instances\n",
    "                    for instance in ner_instances:\n",
    "                        if instance:\n",
    "                            ner_list.append(instance)\n",
    "        elif split == 'Test':\n",
    "            for folder in subdir:  #looping through 'Test for DDI Extraction task' and 'Test for DrugNER task'\n",
    "                folder = glob(\"{}/*\".format(folder))\n",
    "                for subfolder in folder: #looping through 'DrugBank' and 'MedLine'\n",
    "                    subfolder = glob(\"{}/*\".format(subfolder))\n",
    "                    for xml_file in subfolder:\n",
    "                        token_instances, ner_instances, id2word = parse_xml(xml_file, split, id2word, id2ner)\n",
    "                        #print(\"NER_INSTANCES (Test): \", ner_instances)\n",
    "                        data_list = data_list + token_instances\n",
    "                        for instance in ner_instances:\n",
    "                            if instance:\n",
    "                                ner_list.append(instance)\n",
    "    \n",
    "    vocab = list(id2word.values()) #keeping track of unique words in the data\n",
    "    data_df, ner_df = list2df(data_list, ner_list) #turn lists into dataframes\n",
    "    #df1 = data_df[data_df.isnull().any(axis=1)]\n",
    "    display(data_df)\n",
    "    #display(ner_df)\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #    display(data_df)\n",
    "    return data_df, ner_df, vocab, id2word\n",
    "                    \n",
    "def list2df(data_list, ner_list):\n",
    "    data_df = pd.DataFrame.from_records(data_list, columns=['sentence_id', 'token_id', 'char_start_id', 'char_end_id', 'split'])\n",
    "    #data_df = data_df[~data_df['token'].isin(list(string.punctuation))] #remove tokens that are just punctuation \n",
    "    #data_df.drop('token', inplace=True, axis=1) #remove 'token' column since it's not needed anymore\n",
    "    #'inPlace=True' means we are working on the original df, 'axis=1' refers to the column axis\n",
    "    #train_samples = data_df[data_df['split']=='Train'].sample(frac=0.15) #sample 15 % of 'Train'-labeled rows --> SENTENCES\n",
    "    #train_samples.split='Val' #replace those 'Train' labels with 'Val'\n",
    "    #data_df.update(train_samples) #incorporate the modified train samples back into the original dataframe\n",
    "    train_df = data_df[data_df['split']=='Train']\n",
    "    unique_sent_in_train = list(train_df['sentence_id'].unique())\n",
    "    print(\"UNIQUE TRAIN SENT: \", len(unique_sent_in_train))\n",
    "    val_sample_sentences = unique_sent_in_train[:int(len(unique_sent_in_train) * .15)]\n",
    "    print(\"UNIQUE VAL SENT: \", len(val_sample_sentences))\n",
    "    val_df = data_df[data_df['sentence_id'].isin(val_sample_sentences)]\n",
    "    val_df.split='Val'\n",
    "    data_df.update(val_df)\n",
    "    val_df = data_df[data_df.split=='Val']\n",
    "    ner_df = pd.DataFrame.from_records(ner_list, columns=['sentence_id', 'ner_id', 'char_start_id', 'char_end_id'])\n",
    "    counts = ner_df[\"ner_id\"].value_counts()\n",
    "    print(counts)\n",
    "    return data_df, ner_df    \n",
    "                        \n",
    "def parse_xml(xml_file, split, id2word, id2ner):\n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    token_instances = [] #save all token \n",
    "    ner_instances = []\n",
    "    \n",
    "    for elem in root: #loop over sentence tags\n",
    "        if elem.tag == 'sentence':\n",
    "            sent_id = elem.attrib['id'] #get sentence id\n",
    "            text = elem.attrib['text']  #get the sentence as a string of text\n",
    "            text = text.replace('-', ' ') #replaces all hyphens with whitespace for easier split of compound words\n",
    "            char_pos = -1 #variable for keeping track of character-based positions of the words in the sentence\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            for token in nltk_tokens:\n",
    "                char_pos, token_instance, id2word  = get_token_instance(char_pos, sent_id, token, split, id2word)\n",
    "                token_instances.append(token_instance)\n",
    "        for subelem in elem: #looping through children tags (i.e. 'entity', 'pair') of sentence_id\n",
    "            if subelem.tag == 'entity':\n",
    "                ner_instance = get_ner_instance(sent_id, subelem, id2ner)\n",
    "                #print(\"ner_instance (parse_xml): \", ner_instance)\n",
    "                for instance in ner_instance: #loop through list of returned NER instances\n",
    "                    ner_instances.append(instance) #save them individually in the ner_instances list\n",
    "    #print(\"NER_INSTANCES: \", ner_instances)\n",
    "    return token_instances, ner_instances, id2word\n",
    "\n",
    "def get_token_instance(char_pos, sent_id, token, split, id2word):\n",
    "    char_pos += 1\n",
    "    char_start = char_pos\n",
    "    char_end = char_start + len(token)-1\n",
    "    token_id, id2word = map_token_to_id(token, id2word)\n",
    "    token_instance = [sent_id, int(token_id), int(char_start), int(char_end), split]\n",
    "    #print(\"TOKEN INSTANC E: \", token_instance)(\n",
    "    char_pos=char_end+1 #increase by 1 to account for the whitespace between the current and the next word\n",
    "    return char_pos, token_instance, id2word\n",
    "\n",
    "def get_ner_id_as_int(ner_id, id2ner):\n",
    "    for key, value in id2ner.items(): \n",
    "         if ner_id == value: \n",
    "            return key \n",
    "    else:\n",
    "        return \"key doesn't exist\"\n",
    "    \n",
    "\n",
    "def get_ner_instance(sent_id, entity, id2ner):\n",
    "    #Problem of this approach: if a NER might be tokenized differently from the token dataframe\n",
    "    ner_instances = []\n",
    "    #ner_token = entity.attrib['text']\n",
    "    #tokenized_ner = entity.attrib['text'].split() \n",
    "    charOffset = entity.attrib['charOffset']\n",
    "    #HAPPY PATH: if the character span is a single span:\n",
    "    if ';' not in charOffset:\n",
    "        char_start = charOffset.split('-')[0]\n",
    "        char_end = charOffset.split('-')[1]\n",
    "        ner_id = get_ner_id_as_int(entity.attrib['type'], id2ner)\n",
    "        #ner_id = entity.attrib['type'] #getting the label: 'brand', 'drug', 'drug_n' or 'group'\n",
    "        ner_instance = [sent_id, int(ner_id), int(char_start), int(char_end)]\n",
    "        return [ner_instance]\n",
    "    #PATH OF DOOM: for multiword entities with several character spans:\n",
    "    if ';' in charOffset:\n",
    "        for span in charOffset.split(';'):\n",
    "            ner_id = get_ner_id_as_int(entity.attrib['type'], id2ner) #getting the label: 'brand', 'drug', 'drug_n' or 'group'\n",
    "            char_start = span.split('-')[0]\n",
    "            char_end = span.split('-')[1]\n",
    "            ner_instance = [sent_id, int(ner_id), int(char_start), int(char_end)]\n",
    "            ner_instances.append(ner_instance)\n",
    "            #print(\"SPECIAL NER_INSTANCE: \", ner_instance)\n",
    "    return ner_instances\n",
    "    \n",
    "def map_token_to_id(token, id2word):\n",
    "    res = False\n",
    "    for key in id2word: \n",
    "        if(id2word[key] == token):\n",
    "            res = True\n",
    "            return key, id2word\n",
    "    if res == False:\n",
    "        token_id = len(id2word)+1\n",
    "        id2word[token_id] = token\n",
    "    return token_id, id2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE TRAIN SENT:  6905\n",
      "UNIQUE VAL SENT:  1035\n",
      "3    11656\n",
      "1     4254\n",
      "4     1865\n",
      "2      766\n",
      "Name: ner_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/pandas/core/generic.py:5209: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>char_start_id</th>\n",
       "      <th>char_end_id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DDI-DrugBank.d610.s0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DDI-DrugBank.d610.s1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195915</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195916</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195917</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>10890.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195918</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>540.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195919</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>32.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195920</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>47.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195921</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>2433.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195922</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195923</th>\n",
       "      <td>DDI-MedLine.d113.s5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195924</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195925</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>856.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195926</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195927</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195928</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>210.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195929</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>2350.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195930</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>130.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195931</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>47.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195932</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>6768.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195933</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>12327.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195934</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>2563.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195935</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195936</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195937</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>764.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195938</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195939</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195940</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>636.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195941</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195942</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195943</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>510.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195944</th>\n",
       "      <td>DDI-MedLine.d113.s6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195945 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sentence_id  token_id  char_start_id  char_end_id  split\n",
       "0       DDI-DrugBank.d610.s0       1.0            0.0         14.0   Test\n",
       "1       DDI-DrugBank.d610.s0       2.0           16.0         25.0   Test\n",
       "2       DDI-DrugBank.d610.s0       3.0           27.0         28.0   Test\n",
       "3       DDI-DrugBank.d610.s0       4.0           30.0         37.0   Test\n",
       "4       DDI-DrugBank.d610.s0       5.0           39.0         42.0   Test\n",
       "...                      ...       ...            ...          ...    ...\n",
       "195940   DDI-MedLine.d113.s6     636.0          107.0        112.0  Train\n",
       "195941   DDI-MedLine.d113.s6       3.0          114.0        115.0  Train\n",
       "195942   DDI-MedLine.d113.s6     372.0          117.0        120.0  Train\n",
       "195943   DDI-MedLine.d113.s6     510.0          122.0        126.0  Train\n",
       "195944   DDI-MedLine.d113.s6      17.0          128.0        128.0  Train\n",
       "\n",
       "[195945 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df, ner_df, vocab, id2word = parse_data('/home/guserbto@GU.GU.SE/lt2316-h20-aa/DDICorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(data_df, ner_df):\n",
    "    # Should return a tensor containing the ner labels for all samples in each split.\n",
    "    # the tensors should have the following following dimensions:\n",
    "    # (NUMBER_SAMPLES, MAX_SAMPLE_LENGTH)\n",
    "    # NOTE! the labels for each split should be on the GPU\n",
    "    device = torch.device('cuda:1')\n",
    "    \n",
    "    # divide df by splits\n",
    "    df_train = data_df[data_df.split=='Train']\n",
    "    print(\"Unique sent in Train: \", len(list(df_train['sentence_id'].unique()))) \n",
    "    #print(\"df_train size: \", df_train.size)\n",
    "    df_val = data_df[data_df.split=='Val']\n",
    "    #print(\"df_val size: \", df_val.size)\n",
    "    print(\"Unique sent in Val: \", len(list(df_val['sentence_id'].unique()))) \n",
    "    df_test = data_df[data_df.split=='Test']\n",
    "    #print(\"df_test size: \", df_test.size)\n",
    "    print(\"Unique sent in Test: \", len(list(df_test['sentence_id'].unique()))) \n",
    "    \n",
    "    max_sample_length, sample_lengths_dict = get_sample_lengths(data_df)\n",
    "    \n",
    "    #get labels\n",
    "    train_labels = label_tokens(df_train, ner_df, max_sample_length, sample_lengths_dict)\n",
    "    #print(\"train labels:\", len(train_labels))\n",
    "    train_tensor = torch.LongTensor(train_labels)\n",
    "    train_tensor = train_tensor.to(device)\n",
    "    \n",
    "    test_labels = label_tokens(df_test, ner_df, max_sample_length, sample_lengths_dict)\n",
    "    test_tensor = torch.LongTensor(test_labels)\n",
    "    test_tensor = test_tensor.to(device)\n",
    "    \n",
    "    val_labels = label_tokens(df_val, ner_df, max_sample_length, sample_lengths_dict)\n",
    "    val_tensor = torch.LongTensor(val_labels)\n",
    "    val_tensor = val_tensor.to(device)\n",
    "  \n",
    "    print(\"val labels:\", len(val_labels))\n",
    "    print(\"test labels:\", len(test_labels))\n",
    "    print(\"test labels:\", len(train_labels))\n",
    "\n",
    "\n",
    "    return train_tensor, val_tensor, test_tensor\n",
    "\n",
    "def label_tokens(df, ner_df, max_sample_length, sample_lengths_dict):\n",
    "    labels = []\n",
    "    \n",
    "    df_as_list = df.values.tolist()\n",
    "    #print(\"df_as_list[:50]: \", df_as_list[:50])\n",
    "    ner_df_as_list = ner_df.values.tolist()\n",
    "    \n",
    "    #max_sample_length = 165\n",
    "    \n",
    "    sentence_labels = []\n",
    "    match_found_count = 0\n",
    "    for df_row in df_as_list:\n",
    "        sentence_id = df_row[0]\n",
    "        sentence_length = sample_lengths_dict[sentence_id]\n",
    "        #print(\"SENTENCE_LENGTH: \", sentence_length)\n",
    "        match_found = False \n",
    "        for ner_row in ner_df_as_list:\n",
    "            #compare sentence_id, char_start, char_end between df_row and ner_rows: \n",
    "            if df_row[0] == ner_row[0]:\n",
    "                if int(df_row[2]) == ner_row[2] and int(df_row[3]) == ner_row[3]:\n",
    "                    label = ner_row[1]\n",
    "                    match_found = True\n",
    "                    #print(\"match found\", df_row, \"<3\", ner_row) \n",
    "                    sentence_labels.append(label)\n",
    "                    #continue\n",
    "        if match_found == False:\n",
    "            label = 0\n",
    "            sentence_labels.append(label)\n",
    "        if len(sentence_labels) == sentence_length:\n",
    "            #print(\"************SENTENCE DONE************\")\n",
    "            #print(\"SENTENCE_LABELS BEFORE PADDING: \", sentence_labels)\n",
    "            padded_sentence_labels = get_padding(sentence_labels, max_sample_length)\n",
    "            #print(\"PADDED SENTENCE LABELS: \", sentence_labels)\n",
    "            #if padded_sentence_labels is not None:\n",
    "            labels.append(padded_sentence_labels)\n",
    "            #print(\"LABELS AFTER ADDING PADDED SENT: \", labels)\n",
    "            sentence_labels = []\n",
    "            #else:\n",
    "            #    print(\"omg it's a none\", sentence_id)\n",
    "    #print(\"LABELS BEFORE RETURN: \", labels)\n",
    "            \n",
    "    return labels \n",
    "\n",
    "def get_sample_lengths(data_df):\n",
    "    max_sample_length = max(data_df.groupby('sentence_id').size())\n",
    "    sample_lengths = data_df.groupby('sentence_id').size().tolist() \n",
    "    unique_sentences = data_df['sentence_id'].unique() \n",
    "    sentences_list = sorted(unique_sentences) \n",
    "    sample_length_dict = {sentences_list[i]: sample_lengths[i] for i in range(len(sentences_list))} \n",
    "    #display(data_df.groupby('sentence_id').size().nlargest(5))\n",
    "    #display(data_df.groupby('sentence_id').size()) \n",
    "    return max_sample_length, sample_length_dict\n",
    "\n",
    "def get_padding(sentence_labels, max_sample_length):\n",
    "    #print(\"SENTENCE LABELS: \", sentence_labels)\n",
    "    diff = max_sample_length - len(sentence_labels)\n",
    "    #print(\"DIFF: \", diff)\n",
    "    if int(diff) == 0:\n",
    "        #print(\"SENTENCE WITH NO DIFF: \", sentence_labels, \"DIFF: \", diff)\n",
    "        return sentence_labels\n",
    "    else:\n",
    "        padding = [0] * diff\n",
    "        sentence_labels.extend(padding)\n",
    "    return sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sent in Train:  5870\n",
      "Unique sent in Val:  1035\n",
      "Unique sent in Test:  1964\n",
      "val labels: 1035\n",
      "test labels: 1684\n",
      "test labels: 5870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [3, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_y(data_df, ner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_ner_distribution(data_df, ner_df):\n",
    "    \n",
    "    id2ner = {0:'none', 1:'group', 2:'drug_n', 3:'drug', 4:'brand'}\n",
    "     \n",
    "    # divide df by splits and get unique sentences:\n",
    "    df_train = data_df[data_df.split=='Train']\n",
    "    train_sent = list(df_train['sentence_id'].unique())\n",
    "    df_val = data_df[data_df.split=='Val']\n",
    "    val_sent = list(df_val['sentence_id'].unique())\n",
    "    df_test = data_df[data_df.split=='Test']\n",
    "    test_sent = list(df_test['sentence_id'].unique())\n",
    "    \n",
    "    ner_df_as_list = ner_df.values.tolist()\n",
    "    \n",
    "    counts = {'Train': {'group': 0, 'drug_n': 0, 'drug': 0, 'brand':0}, \n",
    "              'Val': {'group': 0, 'drug_n': 0, 'drug': 0, 'brand':0}, \n",
    "              'Test': {'group': 0, 'drug_n': 0, 'drug': 0, 'brand':0}}\n",
    "    for ner in ner_df_as_list:\n",
    "        sent_id = ner[0]\n",
    "        ner_label = id2ner[ner[1]]\n",
    "        if sent_id in train_sent:\n",
    "            counts['Train'][ner_label] += 1\n",
    "        elif sent_id in val_sent:\n",
    "            counts['Val'][ner_label] += 1\n",
    "        elif sent_id in test_sent:\n",
    "            counts['Test'][ner_label] += 1\n",
    "    \n",
    "    #print(counts)\n",
    "    df = pd.DataFrame(counts)\n",
    "    df_to_plot = df.transpose()\n",
    "    df_to_plot.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_split_ner_distribution(data_df, ner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_padding(sentence, max_sample_length):\n",
    "    #print(\"SENTENCE LABELS: \", sentence_labels)\n",
    "    diff = max_sample_length - len(sentence)\n",
    "    #print(\"DIFF: \", diff)\n",
    "    if int(diff) == 0:\n",
    "        #print(\"SENTENCE WITH NO DIFF: \", sentence_labels, \"DIFF: \", diff)\n",
    "        return sentence\n",
    "    else:\n",
    "        padding = [0] * diff\n",
    "        sentence.extend(padding)\n",
    "    return sentence\n",
    "\n",
    "def get_sample_lengths(data_df):\n",
    "    max_sample_length = max(data_df.groupby('sentence_id').size())\n",
    "    sample_lengths = data_df.groupby('sentence_id').size().tolist() \n",
    "    unique_sentences = data_df['sentence_id'].unique() \n",
    "    sentences_list = sorted(unique_sentences) \n",
    "    sample_length_dict = {sentences_list[i]: sample_lengths[i] for i in range(len(sentences_list))} \n",
    "    #display(data_df.groupby('sentence_id').size().nlargest(5))\n",
    "    #display(data_df.groupby('sentence_id').size()) \n",
    "    return max_sample_length, sample_length_dict\n",
    "\n",
    "def extract_features(data_df):\n",
    "    device = torch.device('cuda:1')\n",
    "    print(\"extracting features\")\n",
    "    id2pos = {}\n",
    "    # divide df by splits\n",
    "    df_train = data_df[data_df.split=='Train']\n",
    "    unique_sent_train = list(df_train['sentence_id'].unique())\n",
    "    print(\"Unique sent in Train: \", len(list(df_train['sentence_id'].unique()))) \n",
    "    #print(\"df_train size: \", df_train.size)\n",
    "    df_val = data_df[data_df.split=='Val']\n",
    "    #print(\"df_val size: \", df_val.size)\n",
    "    print(\"Unique sent in Val: \", len(list(df_val['sentence_id'].unique()))) \n",
    "    df_test = data_df[data_df.split=='Test']\n",
    "    #print(\"df_test size: \", df_test.size)\n",
    "    print(\"Unique sent in Test: \", len(list(df_test['sentence_id'].unique()))) \n",
    "    \n",
    "    max_sample_length, sample_length_dict = get_sample_lengths(data_df)\n",
    "    \n",
    "    train_pos, id2pos = get_pos(df_train, max_sample_length, sample_length_dict, id2pos)\n",
    "    test_pos, id2pos = get_pos(df_test, max_sample_length, sample_length_dict, id2pos)\n",
    "    val_pos, id2pos = get_pos(df_val, max_sample_length, sample_length_dict, id2pos)\n",
    "    \n",
    "    print(\"val embeddings:\", len(val_pos))\n",
    "    print(\"test embeddings:\", len(test_pos))\n",
    "    print(\"train embeddings:\", len(train_pos))\n",
    "    \n",
    "    #print(\"train labels:\", len(train_labels))\n",
    "    train_tensor = torch.LongTensor(train_pos)\n",
    "    train_tensor = train_tensor.to(device)\n",
    "    \n",
    "    \n",
    "    test_tensor = torch.LongTensor(test_pos)\n",
    "    test_tensor = test_tensor.to(device)\n",
    "    \n",
    "   \n",
    "    val_tensor = torch.LongTensor(val_pos)\n",
    "    val_tensor = val_tensor.to(device)\n",
    "    \n",
    "    print(\"id2pos: \", id2pos)\n",
    "    \n",
    "    return val_tensor, test_tensor, train_tensor\n",
    "    \n",
    "    #return three tensor of the following dimensions: NUMBER_SAMPLES, MAX_SAMPLE_LENGTH, FEATURE_DIM\n",
    "\n",
    "def get_pos(df, max_sample_length, sample_lengths_dict, id2pos):\n",
    "    \n",
    "    pos_sentences = []\n",
    "    \n",
    "    df_as_list = df.values.tolist()\n",
    "    \n",
    "    \n",
    "    sentence = []\n",
    "    sentence_count = 0\n",
    "    for df_row in df_as_list:\n",
    "        sentence_id = df_row[0]\n",
    "        sentence_length = sample_lengths_dict[sentence_id]\n",
    "        token_id = df_row[1]\n",
    "        sentence.append(token_id)\n",
    "        if len(sentence) == sentence_length:\n",
    "            #print(\"SENT:\", len(sentence))\n",
    "            padded_sent = get_padding(sentence, max_sample_length)\n",
    "            wordified_sent = []\n",
    "            for token_id in padded_sent:\n",
    "                if token_id in id2word.keys():\n",
    "                    wordified_sent.append(id2word[token_id])\n",
    "                else:\n",
    "                    wordified_sent.append(\"PADDXNG\")              \n",
    "            #print(\"WORDIFIED:\", wordified_sent)\n",
    "            pos_tagged_sent = nltk.pos_tag(wordified_sent)\n",
    "            #print(\"POS_TAGGED SENT:\", pos_tagged_sent) \n",
    "            pos_id_sent = []\n",
    "            for pos_tuple in pos_tagged_sent:\n",
    "                pos_id, id2pos = map_pos_to_id(pos_tuple[1], id2pos)\n",
    "                #print(\"pos_tuple[1]\", pos_tuple[1])\n",
    "                pos_id_sent.append(pos_id)\n",
    "            #print(\"POS_ID SENT:\", pos_id_sent)\n",
    "            pos_sentences.append(pos_id_sent)\n",
    "            sentence_count +=1\n",
    "            sentence = []\n",
    "    \n",
    "    return pos_sentences, id2pos\n",
    "\n",
    "def map_pos_to_id(pos, id2pos):\n",
    "    res = False\n",
    "    for key in id2pos:\n",
    "        if(id2pos[key] == pos):\n",
    "            res = True\n",
    "            return key, id2pos\n",
    "    if res == False:\n",
    "        pos_id = len(id2pos)+1\n",
    "        id2pos[pos_id] = pos\n",
    "    return pos_id, id2pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting features\n",
      "Unique sent in Train:  5870\n",
      "Unique sent in Val:  1035\n",
      "Unique sent in Test:  1964\n",
      "val embeddings: 1035\n",
      "test embeddings: 1964\n",
      "train embeddings: 5870\n",
      "id2pos:  {1: 'NN', 2: ':', 3: 'IN', 4: 'NNS', 5: 'CC', 6: 'DT', 7: 'VBN', 8: '.', 9: 'NNP', 10: 'JJ', 11: 'VBZ', 12: 'RB', 13: 'TO', 14: 'PRP$', 15: ',', 16: 'MD', 17: 'VB', 18: 'VBD', 19: 'VBG', 20: 'VBP', 21: 'WDT', 22: 'CD', 23: '(', 24: ')', 25: 'EX', 26: 'JJS', 27: 'WP', 28: 'JJR', 29: 'FW', 30: '#', 31: 'PRP', 32: '$', 33: 'WRB', 34: 'POS', 35: 'RBR', 36: 'UH', 37: 'RP', 38: 'WP$', 39: 'NNPS', 40: 'PDT', 41: 'LS', 42: 'RBS', 43: '``', 44: \"''\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6,  1, 15,  ...,  9,  9,  9],\n",
       "         [ 6, 10,  1,  ...,  9,  9,  9],\n",
       "         [ 9,  9, 22,  ...,  9,  9,  9],\n",
       "         ...,\n",
       "         [ 4,  3, 10,  ...,  9,  9,  9],\n",
       "         [12, 15,  6,  ...,  9,  9,  9],\n",
       "         [ 1,  2,  1,  ...,  9,  9,  9]], device='cuda:1'),\n",
       " tensor([[10,  4,  3,  ...,  9,  9,  9],\n",
       "         [ 6, 12, 10,  ...,  9,  9,  9],\n",
       "         [ 9, 11,  6,  ...,  9,  9,  9],\n",
       "         ...,\n",
       "         [ 6,  1,  1,  ...,  9,  9,  9],\n",
       "         [ 6,  1,  3,  ...,  9,  9,  9],\n",
       "         [ 6,  1, 18,  ...,  9,  9,  9]], device='cuda:1'),\n",
       " tensor([[ 1,  2,  1,  ...,  9,  9,  9],\n",
       "         [ 6, 10,  1,  ...,  9,  9,  9],\n",
       "         [12, 15,  4,  ...,  9,  9,  9],\n",
       "         ...,\n",
       "         [10,  4, 15,  ...,  9,  9,  9],\n",
       "         [ 6,  4, 20,  ...,  9,  9,  9],\n",
       "         [ 6,  1, 18,  ...,  9,  9,  9]], device='cuda:1'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
